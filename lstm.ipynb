{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 08:58:22.403019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 08:58:22.476468: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from openap import aero, nav\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data.loaders import SingleLoader, DisjointLoader, BatchLoader\n",
    "from spektral.layers import GATConv, DiffusionConv, GCNConv\n",
    "from spektral.transforms import LayerPreprocess\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 2016, 2019\n",
    "\n",
    "start = datetime(start_year, 1, 1)\n",
    "end = datetime(end_year, 12, 31)\n",
    "\n",
    "agg_interval = 30  # minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 82.56it/s]\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob.glob(\"data/airport/*.parquet\"))\n",
    "\n",
    "data = {}\n",
    "airports = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    ap = Path(file).stem\n",
    "    airports.append(ap)\n",
    "    \n",
    "    d =  pd.read_parquet(file).sort_values('timeslot')\n",
    "    X = d.drop([\"delay_arrival\", \"delay_departure\"], axis=1)\n",
    "    Y = d[[\"delay_arrival\", \"delay_departure\"]].copy()\n",
    "    T = d[[\"timeslot\"]]\n",
    "    \n",
    "    data[ap] = {}\n",
    "    data[ap]['X'] = X\n",
    "    data[ap]['Y'] = Y\n",
    "    data[ap]['T'] = T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dates = d.query(\"date<'2019-01-01'\").date.unique()\n",
    "# test_dates = d.query(\"date>='2019-01-01'\").date.unique()\n",
    "\n",
    "train_dates = d.query(\"date<'2019-09-01'\").date.unique()\n",
    "test_dates = d.query(\"date>='2019-09-01'\").date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = pd.concat([d[\"X\"] for ap, d in data.items()]).drop([\"timeslot\", \"date\"], axis=1)\n",
    "Y_all = pd.concat([d[\"Y\"] for ap, d in data.items()])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_all, Y_all], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = (pd.concat([X_all, Y_all], axis=1)).corr().round(2)\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.tight_layout()\n",
    "# heat_map = sns.heatmap(\n",
    "#     corr,\n",
    "#     linewidth=1,\n",
    "#     annot=True,\n",
    "#     center=0,\n",
    "#     cmap=\"Spectral_r\",\n",
    "#     vmin=-1,\n",
    "#     vmax=1,\n",
    "#     square=True,\n",
    "#     # xticklabels=ticks,\n",
    "#     # yticklabels=ticks,\n",
    "# )\n",
    "# # heat_map.set_xticklabels(ticks, rotation=30, ha=\"right\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T x AP x F: X_train = (20496, 50, 27) | Y_train = (20496, 50, 2)\n",
      "T x AP x F: X_test  = (2881, 50, 27) | Y_test  = (2881, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "\n",
    "for airport in tqdm(airports):\n",
    "    xy = pd.concat([data[airport][\"X\"], data[airport][\"Y\"]], axis=1)\n",
    "    xy_train = xy.query(\"date in @train_dates\")\n",
    "    xy_test = xy.query(\"date in @test_dates\")\n",
    "\n",
    "    # train: size T x F\n",
    "    x_train = xy_train.drop(columns=[\"timeslot\", \"date\"])\n",
    "    y_train = xy_train[[\"delay_arrival\", \"delay_departure\"]]\n",
    "    X_train.append(scaler.transform(x_train))\n",
    "    Y_train.append(y_train)\n",
    "\n",
    "    # test: size T x F\n",
    "    x_test = xy_test.drop(columns=[\"timeslot\", \"date\"])\n",
    "    y_test = xy_test[[\"delay_arrival\", \"delay_departure\"]]\n",
    "    X_test.append(scaler.transform(x_test))\n",
    "    Y_test.append(y_test)\n",
    "\n",
    "X_train = np.stack(X_train)\n",
    "Y_train = np.stack(Y_train)\n",
    "X_test = np.stack(X_test)\n",
    "Y_test = np.stack(Y_test)\n",
    "\n",
    "# N x T x F\n",
    "X_train = np.swapaxes(X_train, 0, 1)\n",
    "Y_train = np.swapaxes(Y_train, 0, 1)\n",
    "X_test = np.swapaxes(X_test, 0, 1)\n",
    "Y_test = np.swapaxes(Y_test, 0, 1)\n",
    "\n",
    "print(f\"T x AP x F: X_train = {X_train.shape} | Y_train = {Y_train.shape}\")\n",
    "print(f\"T x AP x F: X_test  = {X_test.shape} | Y_test  = {Y_test.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    lookback: int,\n",
    "    lookahead: int,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "\n",
    "    # make windows\n",
    "    idx = 0\n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "\n",
    "    while idx + lookback + lookahead < len(X):\n",
    "        # features\n",
    "        # x = X[idx : idx + lookback, :, :]\n",
    "        x = np.append(\n",
    "            X[idx : idx + lookback, :, :], Y[idx : idx + lookback, :, :], axis=2\n",
    "        )\n",
    "        X_.append(x)\n",
    "\n",
    "        # make labels with multi-horizon\n",
    "        y = Y[idx + lookback : idx + lookback + lookahead, :, :]\n",
    "        Y_.append(y)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    X = np.array(X_)\n",
    "    Y = np.array(Y_)\n",
    "\n",
    "    # Ensure only complete batches are made (remove incomplete ones)\n",
    "    batchCutoff = X.shape[0] - (X.shape[0] % batch_size)\n",
    "\n",
    "    X, Y = (\n",
    "        X[:batchCutoff, :, :, :],\n",
    "        Y[:batchCutoff, :, :, :],\n",
    "    )\n",
    "\n",
    "    X = X.reshape(X.shape[0]*X.shape[2], X.shape[1], X.shape[3])\n",
    "    Y = Y.reshape(Y.shape[0]*Y.shape[2], Y.shape[1], Y.shape[3])\n",
    "\n",
    "    with tf.device(\"CPU\"):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "patience = 5\n",
    "lookback = 8\n",
    "lookahead = 8\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 08:58:27.977674: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 08:58:28.467431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5937 MB memory:  -> device: 0, name: NVIDIA T1000 8GB, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 8, 29), dtype=tf.float64, name=None), TensorSpec(shape=(None, 8, 2), dtype=tf.float64, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 8, 29), dtype=tf.float64, name=None), TensorSpec(shape=(None, 8, 2), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = createDataset(X_train, Y_train, lookback, lookahead, batch_size)\n",
    "test_dataset = createDataset(X_test, Y_test, lookback, lookahead, batch_size)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 2 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm_units = 60\n",
    "\n",
    "n_features = X_train.shape[2] + Y_train.shape[2]  # t-back to t0\n",
    "n_targets = Y_train.shape[2]  # t0 to t+forward\n",
    "\n",
    "n_nodes = len(airports)\n",
    "\n",
    "print(n_features, n_targets, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 8, 29)\n",
      "(64, 60)\n",
      "(64, 16)\n",
      "(64, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(\n",
    "    shape=(lookback, n_features), batch_size=batch_size, name=\"Features\"\n",
    ")\n",
    "print(input.shape)\n",
    "\n",
    "lstm1 = layers.LSTM(lstm_units, dropout=0.1, return_sequences=True, name=\"LSTM1\")(input)\n",
    "lstm2 = layers.LSTM(lstm_units, dropout=0.1, return_sequences=False, name=\"LSTM2\")(lstm1)\n",
    "\n",
    "\n",
    "dense1 = layers.Dense(lstm_units, name=\"Dense1\")(lstm2)\n",
    "dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "print(dense1.shape)\n",
    "\n",
    "dense2 = layers.Dense(n_targets * lookahead, name=\"DenseFinal\")(dense1)\n",
    "print(dense2.shape)\n",
    "\n",
    "output = tf.reshape(\n",
    "    dense2, (batch_size, lookahead, n_targets), name=\"ReshapeFinal\"\n",
    ")\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Features (InputLayer)       [(64, 8, 29)]             0         \n",
      "                                                                 \n",
      " LSTM1 (LSTM)                (64, 8, 60)               21600     \n",
      "                                                                 \n",
      " LSTM2 (LSTM)                (64, 60)                  29040     \n",
      "                                                                 \n",
      " Dense1 (Dense)              (64, 60)                  3660      \n",
      "                                                                 \n",
      " dropout (Dropout)           (64, 60)                  0         \n",
      "                                                                 \n",
      " DenseFinal (Dense)          (64, 16)                  976       \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (64, 8, 2)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,276\n",
      "Trainable params: 55,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=input, outputs=output, name=\"LSTM\")\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=\"mse\")\n",
    "# model.compile(optimizer=Adam(), loss=\"huber_loss\")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 08:58:33.379448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-03-17 08:58:33.527122: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f4064025d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-17 08:58:33.527176: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA T1000 8GB, Compute Capability 7.5\n",
      "2023-03-17 08:58:33.537670: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-17 08:58:33.607611: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-17 08:58:33.644827: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 123s 7ms/step - loss: 71.2404 - val_loss: 78.8526\n",
      "Epoch 2/100\n",
      " 4355/16000 [=======>......................] - ETA: 1:21 - loss: 68.1598"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es],\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "\n",
    "for i, airport in enumerate(airports):\n",
    "    xy = pd.concat([data[airport][\"X\"], data[airport][\"Y\"]], axis=1)\n",
    "    xy_test = xy.query(\"date in @test_dates\").iloc[lookback:]\n",
    "\n",
    "    # Test delay metric, column represent lookahead time\n",
    "    arr_delay = Y_test[lookback:, i, 0]\n",
    "    n_row = len(arr_delay) - lookahead\n",
    "    arr_delays = np.empty((n_row, lookahead))\n",
    "    for k in range(lookahead):\n",
    "        arr_delays[:, k] = arr_delay[k : k + n_row]\n",
    "\n",
    "    dep_delay = Y_test[lookback:, i, 1]\n",
    "    n_row = len(dep_delay) - lookahead\n",
    "    dep_delays = np.empty((n_row, lookahead))\n",
    "    for k in range(lookahead):\n",
    "        dep_delays[:, k] = dep_delay[k : k + n_row]\n",
    "\n",
    "    airport_delays = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame().assign(time=xy_test[\"timeslot\"].values),\n",
    "                pd.DataFrame(\n",
    "                    arr_delays.round().astype(int),\n",
    "                    columns=[f\"arr_{30*n}m\" for n in range(1, lookahead + 1)],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    dep_delays.round().astype(int),\n",
    "                    columns=[f\"dep_{30*n}m\" for n in range(1, lookahead + 1)],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    Y_pred[:, :, i, 0].round().astype(int),\n",
    "                    columns=[f\"arr_est_{30*n}m\" for n in range(1, lookahead + 1)],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    Y_pred[:, :, i, 1].round().astype(int),\n",
    "                    columns=[f\"dep_est_{30*n}m\" for n in range(1, lookahead + 1)],\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        .dropna()\n",
    "        .assign(airport=airport)\n",
    "    )\n",
    "\n",
    "    buffer.append(airport_delays)\n",
    "\n",
    "results = pd.concat(buffer, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv(\"data/network_delay_gnn_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.assign(time=lambda d: pd.to_datetime(d.time)).assign(\n",
    "    date=lambda d: d.time.dt.date\n",
    ")\n",
    "\n",
    "res_arr = dict()\n",
    "res_dep = dict()\n",
    "for i in range(1, lookahead+1):\n",
    "    res_arr[i] = {\n",
    "        \"Lookahead\": f\"{i*30} min\",\n",
    "        \"MAE\": mean_absolute_error(df[f\"arr_{i*30}m\"], df[f\"arr_est_{i*30}m\"]),\n",
    "        \"RMSE\": mean_squared_error(\n",
    "            df[f\"arr_{i*30}m\"], df[f\"arr_est_{i*30}m\"], squared=False\n",
    "        ),\n",
    "        \"R2\": r2_score(df[f\"arr_{i*30}m\"], df[f\"arr_est_{i*30}m\"]),\n",
    "    }\n",
    "    res_dep[i] = {\n",
    "        \"Lookahead\": f\"{i*30} min\",\n",
    "        \"MAE\": mean_absolute_error(df[f\"dep_{i*30}m\"], df[f\"dep_est_{i*30}m\"]),\n",
    "        \"RMSE\": mean_squared_error(\n",
    "            df[f\"dep_{i*30}m\"], df[f\"dep_est_{i*30}m\"], squared=False\n",
    "        ),\n",
    "        \"R2\": r2_score(df[f\"dep_{i*30}m\"], df[f\"dep_est_{i*30}m\"]),\n",
    "    }\n",
    "\n",
    "stats_arr = pd.DataFrame.from_dict(res_arr, orient=\"index\")\n",
    "stats_dep = pd.DataFrame.from_dict(res_dep, orient=\"index\")\n",
    "\n",
    "display(stats_arr)\n",
    "display(stats_dep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delay(airport):\n",
    "    df = results.query('airport == @airport')\n",
    "\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(20, 8), sharex=True, sharey=True)\n",
    "\n",
    "    axes[0].set_title(f\"{airport} - Arrival delay (min)\")\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(df.arr_30m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.arr_est_30m, color=\"r\", alpha=0.8, label=\"Predicted (-30 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "    ax.set_ylim(-20, 80)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(df.arr_60m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.arr_est_60m, color=\"r\", alpha=0.8, label=\"Predicted (-60 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[2]\n",
    "    ax.plot(df.arr_120m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.arr_est_120m, color=\"r\", alpha=0.8, label=\"Predicted (-120 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[3]\n",
    "    ax.plot(df.arr_180m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.arr_est_180m, color=\"r\", alpha=0.8, label=\"Predicted (-180 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[4]\n",
    "    ax.plot(df.arr_240m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.arr_est_240m, color=\"r\", alpha=0.8, label=\"Predicted (-240 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    # ax = axes[5]\n",
    "    # ax.plot(df.arr_300m, alpha=0.9, label=\"Actual\")\n",
    "    # ax.plot(df.arr_est_300m, color=\"r\", alpha=0.8, label=\"Predicted (-300 min)\")\n",
    "    # ax.axhline(0, ls=\":\")\n",
    "    # ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(20, 8), sharex=True, sharey=True)\n",
    "\n",
    "    axes[0].set_title(f\"{airport} - Departure delay (min)\")\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(df.dep_30m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.dep_est_30m, color=\"r\", alpha=0.8, label=\"Predicted (-30 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "    ax.set_ylim(-20, 40)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(df.dep_60m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.dep_est_60m, color=\"r\", alpha=0.8, label=\"Predicted (-60 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[2]\n",
    "    ax.plot(df.dep_120m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.dep_est_120m, color=\"r\", alpha=0.8, label=\"Predicted (-120 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[3]\n",
    "    ax.plot(df.dep_180m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.dep_est_180m, color=\"r\", alpha=0.8, label=\"Predicted (-180 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    ax = axes[4]\n",
    "    ax.plot(df.dep_240m, alpha=0.9, label=\"Actual\")\n",
    "    ax.plot(df.dep_est_240m, color=\"r\", alpha=0.8, label=\"Predicted (-240 min)\")\n",
    "    ax.axhline(0, ls=\":\")\n",
    "    ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    # ax = axes[5]\n",
    "    # ax.plot(df.dep_300m, alpha=0.9, label=\"Actual\")\n",
    "    # ax.plot(df.dep_est_300m, color=\"r\", alpha=0.8, label=\"Predicted (-300 min)\")\n",
    "    # ax.axhline(0, ls=\":\")\n",
    "    # ax.legend(loc=\"upper left\", borderaxespad=0.2, ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_delay(\"EHAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdelay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15313c67d3a8b1a23131636eeae4040c21f01b2780f62ac1a4a8162a5b70d34f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
